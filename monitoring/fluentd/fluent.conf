# Fluentd configuration for Rust Security Workspace log aggregation
# This configuration handles security audit logs, application logs, and system logs

# Input sources
<source>
  @type tail
  @id auth_service_logs
  path /var/log/auth-service/*.log
  pos_file /var/log/fluentd/auth-service.log.pos
  tag auth.service
  <parse>
    @type json
    time_key timestamp
    time_format %Y-%m-%dT%H:%M:%S.%LZ
  </parse>
  refresh_interval 5
</source>

<source>
  @type tail
  @id policy_service_logs
  path /var/log/policy-service/*.log
  pos_file /var/log/fluentd/policy-service.log.pos
  tag policy.service
  <parse>
    @type json
    time_key timestamp
    time_format %Y-%m-%dT%H:%M:%S.%LZ
  </parse>
  refresh_interval 5
</source>

<source>
  @type tail
  @id security_audit_logs
  path /var/log/security-audit/*.log
  pos_file /var/log/fluentd/security-audit.log.pos
  tag security.audit
  <parse>
    @type json
    time_key timestamp
    time_format %Y-%m-%dT%H:%M:%S.%LZ
  </parse>
  refresh_interval 1
</source>

# Docker container logs (if running in containers)
<source>
  @type tail
  @id docker_auth_service
  path /var/lib/docker/containers/*/auth-service-*.log
  pos_file /var/log/fluentd/docker-auth.log.pos
  tag docker.auth
  <parse>
    @type json
    time_key time
    time_format %Y-%m-%dT%H:%M:%S.%LZ
  </parse>
</source>

# System logs for security events
<source>
  @type systemd
  @id systemd_security
  matches [{ "_SYSTEMD_UNIT": "auth-service.service" }, { "_SYSTEMD_UNIT": "policy-service.service" }]
  tag systemd.security
  <storage>
    @type local
    persistent true
    path /var/log/fluentd/systemd-security.pos
  </storage>
  <entry>
    field_map {"MESSAGE": "message", "_HOSTNAME": "hostname", "_SYSTEMD_UNIT": "unit"}
    fields_strip_underscores true
    fields_lowercase true
  </entry>
</source>

# Filters for log processing and enrichment
<filter auth.service>
  @type record_transformer
  <record>
    service_name auth-service
    log_type application
    environment "#{ENV['ENVIRONMENT'] || 'development'}"
    cluster "#{ENV['CLUSTER_NAME'] || 'local'}"
    namespace "#{ENV['NAMESPACE'] || 'default'}"
  </record>
</filter>

<filter policy.service>
  @type record_transformer
  <record>
    service_name policy-service
    log_type application
    environment "#{ENV['ENVIRONMENT'] || 'development'}"
    cluster "#{ENV['CLUSTER_NAME'] || 'local'}"
    namespace "#{ENV['NAMESPACE'] || 'default'}"
  </record>
</filter>

<filter security.audit>
  @type record_transformer
  <record>
    service_name security-audit
    log_type security_audit
    environment "#{ENV['ENVIRONMENT'] || 'development'}"
    cluster "#{ENV['CLUSTER_NAME'] || 'local'}"
    namespace "#{ENV['NAMESPACE'] || 'default'}"
    retention_years 7
  </record>
</filter>

# Security event classification and enrichment
<filter security.audit>
  @type parser
  key_name message
  reserve_data true
  <parse>
    @type json
  </parse>
</filter>

<filter security.audit>
  @type record_transformer
  enable_ruby true
  <record>
    # Calculate risk score based on event type and severity
    calculated_risk_score ${
      case record["severity"]
      when "critical"
        90 + (record["risk_score"] || 0) / 10
      when "high"
        70 + (record["risk_score"] || 0) / 10
      when "medium"
        40 + (record["risk_score"] || 0) / 10
      when "low"
        10 + (record["risk_score"] || 0) / 10
      else
        record["risk_score"] || 0
      end
    }
    
    # Add geographic enrichment placeholder
    geo_country ${record["ip_address"] ? "LOOKUP_COUNTRY" : "unknown"}
    geo_city ${record["ip_address"] ? "LOOKUP_CITY" : "unknown"}
    
    # Add threat intelligence placeholder
    threat_intel_score ${record["ip_address"] ? "LOOKUP_THREAT_SCORE" : 0}
    
    # Compliance classification
    compliance_relevant ${
      ["authentication_failure", "token_revoked", "suspicious_activity", 
       "privilege_escalation", "data_access"].include?(record["event_type"]) ? "yes" : "no"
    }
  </record>
</filter>

# PII scrubbing filter
<filter **>
  @type grep
  <exclude>
    key message
    pattern /password|secret|token|key|credential/i
  </exclude>
</filter>

<filter **>
  @type record_transformer
  enable_ruby true
  <record>
    # Scrub potential PII from logs
    message ${
      msg = record["message"] || ""
      # Remove potential credit card numbers
      msg = msg.gsub(/\b\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}\b/, "****-****-****-****")
      # Remove potential SSNs
      msg = msg.gsub(/\b\d{3}-\d{2}-\d{4}\b/, "***-**-****")
      # Remove potential email addresses (keep domain for analysis)
      msg = msg.gsub(/\b[A-Za-z0-9._%+-]+@([A-Za-z0-9.-]+\.[A-Z|a-z]{2,})\b/, "****@\\1")
      msg
    }
  </record>
</filter>

# Rate limiting for high-volume logs
<filter auth.service>
  @type throttle
  group_key client_id
  group_bucket_period_s 60
  group_bucket_limit 1000
  group_reset_rate_s 0.1
</filter>

# Output configurations
# Security audit logs - high retention, secure storage
<match security.audit>
  @type copy
  
  # Primary storage - Elasticsearch for search and analysis
  <store>
    @type elasticsearch
    @id elasticsearch_security_audit
    host "#{ENV['ELASTICSEARCH_HOST'] || 'elasticsearch'}"
    port "#{ENV['ELASTICSEARCH_PORT'] || 9200}"
    user "#{ENV['ELASTICSEARCH_USER']}"
    password "#{ENV['ELASTICSEARCH_PASSWORD']}"
    scheme "#{ENV['ELASTICSEARCH_SCHEME'] || 'http'}"
    ssl_verify "#{ENV['ELASTICSEARCH_SSL_VERIFY'] || 'false'}"
    
    index_name security-audit-%Y.%m.%d
    type_name _doc
    
    # Template for security audit index
    template_name security_audit_template
    template_file /fluentd/etc/templates/security_audit_template.json
    template_overwrite true
    
    # Buffer configuration for reliability
    <buffer time>
      @type file
      path /var/log/fluentd/buffer/security_audit
      timekey 3600
      timekey_wait 60
      chunk_limit_size 32MB
      total_limit_size 1GB
      flush_mode interval
      flush_interval 30s
      retry_type exponential_backoff
      retry_wait 1s
      retry_max_interval 60s
      retry_timeout 3600s
      overflow_action drop_oldest_chunk
    </buffer>
  </store>
  
  # Backup storage - S3 for long-term retention
  <store>
    @type s3
    @id s3_security_audit_backup
    aws_key_id "#{ENV['AWS_ACCESS_KEY_ID']}"
    aws_sec_key "#{ENV['AWS_SECRET_ACCESS_KEY']}"
    s3_bucket "#{ENV['S3_SECURITY_AUDIT_BUCKET'] || 'security-audit-logs'}"
    s3_region "#{ENV['AWS_REGION'] || 'us-east-1'}"
    
    path security-audit/%Y/%m/%d/
    s3_object_key_format %{path}%{time_slice}_%{index}.%{file_extension}
    
    # Compression and encryption
    store_as gzip_command
    <format>
      @type json
    </format>
    
    <buffer time>
      @type file
      path /var/log/fluentd/buffer/s3_security_audit
      timekey 3600
      timekey_wait 60
      chunk_limit_size 256MB
    </buffer>
  </store>
  
  # Real-time alerting - send critical events to alerting system
  <store>
    @type http
    @id security_alert_webhook
    endpoint "#{ENV['SECURITY_ALERT_WEBHOOK_URL']}"
    http_method post
    
    <format>
      @type json
    </format>
    
    <filter>
      tag security.audit
      severity critical,high
    </filter>
    
    <buffer>
      @type memory
      flush_mode immediate
      retry_type exponential_backoff
      retry_wait 1s
      retry_max_interval 30s
      retry_timeout 300s
    </buffer>
  </store>
</match>

# Application logs - standard retention
<match {auth.service,policy.service}>
  @type elasticsearch
  @id elasticsearch_application_logs
  host "#{ENV['ELASTICSEARCH_HOST'] || 'elasticsearch'}"
  port "#{ENV['ELASTICSEARCH_PORT'] || 9200}"
  user "#{ENV['ELASTICSEARCH_USER']}"
  password "#{ENV['ELASTICSEARCH_PASSWORD']}"
  scheme "#{ENV['ELASTICSEARCH_SCHEME'] || 'http'}"
  ssl_verify "#{ENV['ELASTICSEARCH_SSL_VERIFY'] || 'false'}"
  
  index_name application-logs-%Y.%m.%d
  type_name _doc
  
  <buffer time>
    @type file
    path /var/log/fluentd/buffer/application_logs
    timekey 3600
    timekey_wait 60
    chunk_limit_size 64MB
    total_limit_size 2GB
    flush_mode interval
    flush_interval 60s
    retry_type exponential_backoff
    retry_wait 2s
    retry_max_interval 120s
    retry_timeout 1800s
  </buffer>
</match>

# System logs
<match systemd.security>
  @type elasticsearch
  @id elasticsearch_system_logs
  host "#{ENV['ELASTICSEARCH_HOST'] || 'elasticsearch'}"
  port "#{ENV['ELASTICSEARCH_PORT'] || 9200}"
  user "#{ENV['ELASTICSEARCH_USER']}"
  password "#{ENV['ELASTICSEARCH_PASSWORD']}"
  scheme "#{ENV['ELASTICSEARCH_SCHEME'] || 'http'}"
  ssl_verify "#{ENV['ELASTICSEARCH_SSL_VERIFY'] || 'false'}"
  
  index_name system-logs-%Y.%m.%d
  type_name _doc
  
  <buffer time>
    @type file
    path /var/log/fluentd/buffer/system_logs
    timekey 3600
    timekey_wait 60
    chunk_limit_size 32MB
    flush_mode interval
    flush_interval 120s
  </buffer>
</match>

# Catch-all for unmatched logs
<match **>
  @type stdout
  @id stdout_fallback
  <format>
    @type json
  </format>
</match>

# Monitoring and health checks
<source>
  @type monitor_agent
  bind 0.0.0.0
  port 24220
  tag fluentd.monitor
</source>

# Prometheus metrics
<source>
  @type prometheus
  bind 0.0.0.0
  port 24231
  metrics_path /metrics
</source>

<source>
  @type prometheus_monitor
  <labels>
    host "#{Socket.gethostname}"
    environment "#{ENV['ENVIRONMENT'] || 'development'}"
  </labels>
</source>

# Log rotation and cleanup
<system>
  log_level "#{ENV['FLUENTD_LOG_LEVEL'] || 'info'}"
  suppress_repeated_stacktrace true
  emit_error_log_interval 30s
  suppress_config_dump true
  
  <log>
    format json
    time_format %Y-%m-%dT%H:%M:%S.%LZ
  </log>
</system>
