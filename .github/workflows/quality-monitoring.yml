name: Continuous Quality Monitoring

permissions:
  contents: read

concurrency:
  group: quality-monitoring-${{ github.ref }}
  cancel-in-progress: true

on:
  schedule:
    # Run quality monitoring every 6 hours
    - cron: '0 */6 * * *'
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch: # Manual trigger

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1
  QUALITY_THRESHOLD: 95
  TARGET_QUALITY_SCORE: 97

jobs:
  quality-dashboard:
    name: Quality Metrics Dashboard
    runs-on: ubuntu-latest
    
    outputs:
      quality_score: ${{ steps.calculate_score.outputs.quality_score }}
      trend_direction: ${{ steps.calculate_score.outputs.trend }}
      critical_issues: ${{ steps.calculate_score.outputs.critical_issues }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for trend analysis
        
    - name: Setup Rust toolchain
      uses: dtolnay/rust-toolchain@stable
      with:
        components: rustfmt, clippy
        
    - name: Cargo cache
      uses: Swatinem/rust-cache@v2
        
    - name: Install quality analysis tools
      run: |
        cargo install --locked scc tokei cargo-audit cargo-geiger cargo-machete || true
        
    - name: Run comprehensive quality analysis
      run: |
        mkdir -p quality-reports
        
        # Code metrics
        echo "=== Collecting Code Metrics ==="
        tokei --output json > quality-reports/code-metrics.json
        scc --by-file --format json > quality-reports/complexity-metrics.json
        
        # Security analysis
        cargo audit --format json > quality-reports/security-audit.json || echo '{"vulnerabilities": []}' > quality-reports/security-audit.json
        cargo geiger --format json > quality-reports/unsafe-analysis.json || echo '{"used": []}' > quality-reports/unsafe-analysis.json
        
        # Code quality checks
        cargo fmt --all -- --check > quality-reports/format-check.txt 2>&1 || echo "Format issues detected"
        cargo clippy --all-targets --all-features --message-format json > quality-reports/clippy-results.json 2>&1 || echo "Clippy issues detected"
        
        # Dependency analysis
        cargo machete > quality-reports/unused-deps.txt 2>&1 || echo "No unused dependencies"
        
    - name: Calculate quality score
      id: calculate_score
      run: |
        python3 << 'EOF'
        import json
        import os
        import subprocess
        from pathlib import Path
        
        def load_json_safe(filepath):
            try:
                with open(filepath, 'r') as f:
                    return json.load(f)
            except:
                return {}
        
        # Load metrics
        code_metrics = load_json_safe('quality-reports/code-metrics.json')
        complexity_metrics = load_json_safe('quality-reports/complexity-metrics.json')
        security_audit = load_json_safe('quality-reports/security-audit.json')
        unsafe_analysis = load_json_safe('quality-reports/unsafe-analysis.json')
        
        # Calculate base scores
        scores = {}
        
        # Code organization score (25 points)
        total_lines = sum(lang.get('code', 0) for lang in code_metrics.get('languages', {}).values())
        avg_complexity = 0
        if complexity_metrics:
            complexities = [file.get('complexity', 0) for file in complexity_metrics if isinstance(file, dict)]
            avg_complexity = sum(complexities) / len(complexities) if complexities else 0
        
        if avg_complexity <= 5:
            scores['organization'] = 25
        elif avg_complexity <= 10:
            scores['organization'] = 20
        else:
            scores['organization'] = 15
        
        # Security score (25 points)  
        vulnerabilities = security_audit.get('vulnerabilities', [])
        critical_vulns = len([v for v in vulnerabilities if v.get('advisory', {}).get('severity') == 'critical'])
        high_vulns = len([v for v in vulnerabilities if v.get('advisory', {}).get('severity') == 'high'])
        
        if critical_vulns == 0 and high_vulns == 0:
            scores['security'] = 25
        elif critical_vulns == 0 and high_vulns <= 2:
            scores['security'] = 20
        else:
            scores['security'] = 10
        
        # Code quality score (25 points)
        format_issues = Path('quality-reports/format-check.txt').exists()
        try:
            clippy_result = subprocess.run(['cargo', 'clippy', '--all-targets', '--all-features', '--', '-D', 'warnings'], 
                                         capture_output=True, text=True)
            clippy_clean = clippy_result.returncode == 0
        except:
            clippy_clean = False
        
        if not format_issues and clippy_clean:
            scores['quality'] = 25
        elif not format_issues or clippy_clean:
            scores['quality'] = 20
        else:
            scores['quality'] = 15
        
        # Performance score (25 points) - based on unsafe code usage
        unsafe_blocks = len(unsafe_analysis.get('used', []))
        if unsafe_blocks == 0:
            scores['performance'] = 25
        elif unsafe_blocks <= 5:
            scores['performance'] = 20
        else:
            scores['performance'] = 15
        
        # Calculate total score
        total_score = sum(scores.values())
        
        # Determine trend (simplified - compare with target)
        trend = "stable"
        if total_score >= 97:
            trend = "excellent"
        elif total_score >= 95:
            trend = "good"
        elif total_score < 90:
            trend = "declining"
        
        # Count critical issues
        critical_issues = critical_vulns + (1 if not clippy_clean else 0) + (1 if format_issues else 0)
        
        print(f"Quality Analysis Complete:")
        print(f"- Organization: {scores['organization']}/25")
        print(f"- Security: {scores['security']}/25") 
        print(f"- Code Quality: {scores['quality']}/25")
        print(f"- Performance: {scores['performance']}/25")
        print(f"- Total Score: {total_score}/100")
        print(f"- Trend: {trend}")
        print(f"- Critical Issues: {critical_issues}")
        
        # Set GitHub outputs
        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            f.write(f"quality_score={total_score}\n")
            f.write(f"trend={trend}\n")
            f.write(f"critical_issues={critical_issues}\n")
        EOF
        
    - name: Generate quality report
      run: |
        cat > quality-reports/QUALITY_DASHBOARD.md << EOF
        # Code Quality Dashboard - $(date -u +"%Y-%m-%d %H:%M:%S UTC")
        
        **Commit**: ${{ github.sha }}
        **Branch**: ${{ github.ref_name }}
        **Quality Score**: ${{ steps.calculate_score.outputs.quality_score }}/100
        **Trend**: ${{ steps.calculate_score.outputs.trend }}
        **Critical Issues**: ${{ steps.calculate_score.outputs.critical_issues }}
        
        ## Quality Status
        
        $(if [ "${{ steps.calculate_score.outputs.quality_score }}" -ge "97" ]; then
            echo "üü¢ **EXCELLENT** - Industry-leading code quality maintained"
        elif [ "${{ steps.calculate_score.outputs.quality_score }}" -ge "95" ]; then
            echo "üü° **GOOD** - High quality standards maintained"
        elif [ "${{ steps.calculate_score.outputs.quality_score }}" -ge "90" ]; then
            echo "üü† **ACCEPTABLE** - Quality standards met with room for improvement"
        else
            echo "üî¥ **NEEDS ATTENTION** - Quality below acceptable thresholds"
        fi)
        
        ## Metrics Breakdown
        
        ### Code Organization
        - Lines of Code: $(jq -r '.Rust.code // 0' quality-reports/code-metrics.json 2>/dev/null || echo "N/A")
        - Average Complexity: $(jq -r 'map(select(.language == "Rust") | .complexity) | add / length' quality-reports/complexity-metrics.json 2>/dev/null || echo "N/A")
        
        ### Security Posture  
        - Critical Vulnerabilities: $(jq -r '.vulnerabilities | map(select(.advisory.severity == "critical")) | length' quality-reports/security-audit.json 2>/dev/null || echo "0")
        - High Vulnerabilities: $(jq -r '.vulnerabilities | map(select(.advisory.severity == "high")) | length' quality-reports/security-audit.json 2>/dev/null || echo "0")
        - Unsafe Code Blocks: $(jq -r '.used | length' quality-reports/unsafe-analysis.json 2>/dev/null || echo "0")
        
        ### Code Quality
        - Format Compliance: $([ -s quality-reports/format-check.txt ] && echo "‚ùå Issues found" || echo "‚úÖ Clean")
        - Linting Status: $(cargo clippy --all-targets --all-features -- -D warnings >/dev/null 2>&1 && echo "‚úÖ Clean" || echo "‚ùå Issues found")
        
        ## Action Items
        
        $(if [ "${{ steps.calculate_score.outputs.critical_issues }}" -gt "0" ]; then
            echo "### üö® Critical Issues Requiring Immediate Attention"
            echo "- Review security audit results"
            echo "- Fix linting/formatting issues"
            echo "- Update this dashboard after fixes"
        else
            echo "### ‚úÖ No Critical Issues"
            echo "Quality standards are being maintained excellently."
        fi)
        
        ---
        *Automated Quality Monitoring - Next check in 6 hours*
        EOF
        
    - name: Upload quality reports
      uses: actions/upload-artifact@v3
      with:
        name: quality-dashboard-${{ github.run_number }}
        path: quality-reports/
        
    - name: Quality gate check
      run: |
        SCORE=${{ steps.calculate_score.outputs.quality_score }}
        CRITICAL=${{ steps.calculate_score.outputs.critical_issues }}
        
        echo "Quality Score: $SCORE/100 (Threshold: $QUALITY_THRESHOLD)"
        echo "Critical Issues: $CRITICAL"
        
        if [ "$SCORE" -lt "$QUALITY_THRESHOLD" ]; then
            echo "‚ùå Quality score $SCORE is below threshold $QUALITY_THRESHOLD"
            exit 1
        fi
        
        if [ "$CRITICAL" -gt "0" ]; then
            echo "‚ùå $CRITICAL critical issues must be resolved"
            exit 1
        fi
        
        echo "‚úÖ Quality gate passed successfully"

  quality-alerts:
    name: Quality Regression Alerts
    runs-on: ubuntu-latest
    needs: quality-dashboard
    if: needs.quality-dashboard.outputs.quality_score < 95 || needs.quality-dashboard.outputs.critical_issues > 0
    
    steps:
    - name: Quality regression alert
      run: |
        echo "üö® QUALITY REGRESSION DETECTED"
        echo "Current Score: ${{ needs.quality-dashboard.outputs.quality_score }}/100"
        echo "Critical Issues: ${{ needs.quality-dashboard.outputs.critical_issues }}"
        echo "Trend: ${{ needs.quality-dashboard.outputs.trend }}"
        echo ""
        echo "Immediate action required to maintain code quality standards."
        
        # In a real environment, this would send alerts via:
        # - Slack notifications
        # - Email alerts to team leads
        # - GitHub issue creation
        # - Dashboard updates
